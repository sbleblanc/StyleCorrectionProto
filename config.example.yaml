---
# Global configurations
!all_config
global_conf: !global
  # Execution Mode :
  #   - hd5_gen : Create .h5 dataset from the compressed tar.gz raw text
  #   - gen_split : Create a train/valid split for a .h5 dataset. From 1 to 20 splits can be generated for a single
  #                 dataset. At least 1 must be generated.
  #   - vocab_dump : Dump de vocab used for a .h5 dataset
  #   - pretrain : Launch pretraining of a model
  #   - finetune : Launch finetune of a model
  #   - eval : To quickly text translations on a manual list of candidates
  #   - inference : Process sentences using a trained model
  mode: gen_split
  # If the training process should use multiple cuda gpus if available
  multi_gpu: False


# Configurations for model evaluation during training
eval: !eval
  # How many steps between each evaluation
  interval: 250
  # Number of batches from the validation set to use for evaluation
  num_valid_batch: 100


# Configuration for the transformer sequence-to-sequence model
transformer_s2s: !TransformerS2S
  # Size of the embeddings (shared enc/dec)
  emb_dim: 768
  # Number of attention heads (for enc. and dec.)
  n_head: 8
  # Size of the feed-foward layers (for enc. and dec.)
  ff_dim: 4096
  # Number of encoder layers
  num_enc_layers: 6
  # Number of decoder layers
  num_dec_layers: 6
  # The activation function of encoder/decoder intermediate layer, relu or gelu
  activation: relu


# Configuration for the generation of the h5 dataset
hd5_gen: !hd5_gen
  # Path for the generated .h5 file
  h5_fn: temp/datasets/gec_combined_real_gen936k_mp_mn_homo.bpe.h5
  # Path to the compressed tar.gz raw corpus.
  # The corpus should be seperated in sentences and tokenized as there is no default preprossessing (can be easily implemented)
  # Don't combined the corpus in a big .txt before compressing, leave as multiple files and combine by compressing in tar.gz
  corpus_tar_gz: temp/datasets/gec_combined_real_gen936k_mp_mn_homo_complete.tar.gz
  # Top k documents to grab from the tar.gz corpus, 0 for all
  topk: 0
  # Max length of sentences to consider
  max_len: 9999999
  # Additional special tokens to add to the vocabulary. Used to create parallel datasets.
  additional_tokens:
    - '<split>'
  # Ratio of sentences to take as a validation set
  # if valid_ratio is :
  #   - < 1.0, the percentage of random sentences to reserve for the validation set
  #   - > 1.0, the first <valid_ratio> sentences are used for the training set, the rest becomes the validation set
  valid_ratio: 4097416.0


# Configuration for the dump of the vocabulary to use.
vocab_dump: !vocab_dump
  hd5_dataset: !hd5_dataset
    # Path to the h5 dataset
    h5_fn: temp/datasets/pretrain.h5
    # Path for the vocabulary h5 file to generate
    vocab_fn: temp/datasets/pretrain_vocab.h5
    # Which train/valid split to use from the h5 dataset
    valid_split_id: 0
  # Top k most frequent words to keep as long as the frequency is above the min_freq
  vocab_topk: 50000
  # Minimum frequency of a word to be included
  min_freq: 0


# Configuration for the pretraining of a model
pretrain: !pretrain
  # Optimizer to use : 'adam' or 'sgd' (see 'optimizer' configuration group)
  optimizer: adam
  hd5_dataset: !hd5_dataset
    # Path to the h5 dataset
    h5_fn: temp/datasets/bcu_enwiki_30000_bpe.h5
    # Path to the h5 vocabulary to use
    vocab_fn: temp/datasets/bcu_enwiki_30000_bpe_vocab.h5
    # Which train/valid split to use from the h5 dataset
    valid_split_id: 0
    # Whether the sentences should be grouped by length (more efficient training)
    group_by_len: False
  # Max length of a sentence to process (0 to consider all)
  max_sent_len: 0
  # Pretraing algorithm to use : 'mass' or 'bart'
  algo: mass
  # Configure the number of training iterations
  training_max: !training_max
    # What metric to use to stop the training : 'steps' or 'epoch'
    use: steps
    # Amount of <use> iterations (e.g. use='steps', will stop after 1000000 optimizer steps)
    amount: 1000000
  # Configuration for the checkpoints
  model_files: !model_files
    # Path for the best checkpoint file
    best_model: temp/models/best_bpe_pretrained.pkl
    # Path for the current checkpoint file
    current_model: temp/models/current_bpe_pretrained.pkl
    # Which checkpoint to use if training restarts : 'current' or 'best'
    resume_from: current
  # Tokens Per Batch to use. Set mttpb with the same value.
  tpb: 10000
  mttpb: 10000


# Configuration for the fixed bpe preprocessing (fastBPE)
preprocess: !preprocess
  # Path to the codes file generated by fastBPE
  bpe_codes_fn: temp/datasets/bcu_enwiki.30000.codes
  # Path to the vocab file generated by fastBPE
  bpe_vocab_fn: temp/datasets/bcu_enwiki_spacy.30000.bpe.vocab


# Configuration for the GLEU evalution
gleu: !gleu
  # Paths to the text files containing the reference corrections
  refs:
    - temp/jfleg/dev.ref0
    - temp/jfleg/dev.ref1
    - temp/jfleg/dev.ref2
    - temp/jfleg/dev.ref3
  # Path to the text file containing the source sentences
  src: temp/jfleg/dev.src
  # Max order of n-grams to extract
  n: 4
  # Number of iterations
  iter: 500
  # Get result per sentences
  sent: False
  # Preprocess the source sentences before using in seq2seq model
  preprocess: True


# Configuration for the finetuning of a model
finetune: !finetune
  # Optimizer to use : 'adam' or 'sgd' (see 'optimizer' configuration group)
  optimizer: adam
  # Configuration for the checkpoints
  model_files: !model_files
    # Path for the best checkpoint file
    best_model: temp/models/best_gleu_finetune.pkl
    # Path for the current checkpoint file
    current_model: temp/models/current_gleu_finetune.pkl
    # Which checkpoint to use if training restarts : 'current' or 'best'
    resume_from: current
  # Path to the pretrained model (checkpoint generated during pretrain)
  pretrain_model_fn: temp/Notebook/best_gec_combined_real_gen_mp_mn_gbl_ft_femb_gen800k_homo_1e4_300k.pkl
  # Max length of sentences to consider (used to fix some OOM errors)
  max_sent_len: 500
  # What part from the model to freeze.
  #  - None : Every weight is updated
  #  - emb : The embeddings are frozen
  #  - enc : The embeddings and the encoder are frozen
  #  - encdec : The embeddings, the encoder and the decoder are frozen
  freeze: None
  # What metric to use to get the best model : valid or gleu
  best_criteria: gleu
  # Configure the number of training iterations
  training_max: !training_max
    # What metric to use to stop the training : 'steps' or 'epoch'
    use: steps
    # Amount of <use> iterations (e.g. use='steps', will stop after 1000000 optimizer steps)
    amount: 1000
  hd5_dataset: !hd5_dataset
    # Path to the h5 vocabulary to use
    vocab_fn: temp/datasets/bcu_enwiki_30000_bpe_vocab.h5
    # Path to the h5 dataset
    h5_fn: temp/datasets/gec_combined.bpe.h5
    # Which train/valid split to use from the h5 dataset
    valid_split_id: 0
    # Alpha to use for Laplace smoothing of word counts
    smoothing_alpha: 1
    # Whether the sentences should be grouped by length (more efficient training)
    group_by_len: True
  # Configuration for finetuning dataset preprocessing
  dataset: !finetune_dataset
    # Which type of dataset preprocessing to use :
    #  - ca : Dynamically noise source sentence. "hd5_dataset" should be clean sentences.
    #  - parallel : Assumes the dataset in "hd5_dataset" contains source - target sentences
    #               combined in a single string with a splitting token in between (e.g. "helo <split> hello")
    to_use: parallel
    # Configuration for the parallel preprocessing
    parallel: !parallel
      # Should the source - target be inverted
      reverse: False
      # What token to use to split sentence into source - target pair
      split_token: <split>
    # Configuration for the manual noising preprocessing
    #   - replace_prob + del_prob + ins_prob + keep_prob + mask_prob = 1.0
    #   - 0.0 <= shuffle_prob <= 1.0
    ca: !ca
      # Probability to replace a word by a random one
      replace_prob: 0.1
      # Probability to delete a word
      del_prob: 0.1
      # Probability to insert a random word
      ins_prob: 0.1
      # Probability of keeping a word
      keep_prob: 0.2
      # Probability of replacing a word by the <mask> token
      mask_prob: 0.5
      # Probability to shuffle the sentence
      shuffle_prob: 0.1
      # What shuffler to use if the sentence must be shuffled :
      #  - chunk : Randomly moves a chunk of the sentence to a random location
      #  - normal : Applies a gaussian noise to the order of the words
      shuffler: chunk
      # Sigma to use when using the "normal" shuffler. Bigger sigma will give bigger shuffle.
      sigma: 0.5
      # Minimum length ratio of the chunk to swap
      min_chunk_ratio: 0.3
      # Maximum length ratio of the chunk to swap
      max_chunk_ratio: 0.5
    # Tokens Per Batch to use
    tpb: 4000


# Configuration for the optimizers
optimizer: !optimizer
  # Value to use for gradient normalization
  grad_clip_norm: 1.
  # Configuration for the learning rate scheduler
  scheduler: !scheduler
    # What scheduler to use :
    #  - none : Fixed learning rate
    #  - one_cycle : One Cycle Learning rate scheduler. Sharply increases the learning rate to a maximum
    #                and then slowly decreases the learning rate until reaching a minimum.
    use: one_cycle
    # Configuration for PyTorch's One Cycle Scheduler
    # https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.OneCycleLR
    one_cycle: !one_cycle
      max_lr: 1.e-4
      # For div_factor
      initial_lr_div: 25.0
      # For final_div_factor
      final_lr_div: 1.e+10
      # Used to compute pct_start
      warmup_steps: 10000
      #Should be the same as the training_max value.
      total_steps: 1000000
      base_momentum: 0.
      max_momentum: 0.
      anneal_strategy: linear
  # PyTorch Adam optimizer configuration
  adam: !adam
    lr: 1.e-4
    beta_1: 0.9
    beta_2: 0.999
    eps: 1.e-08
    weight_decay: 1.e-2
  # PyTorch SGD optimizer configuration
  sgd: !sgd
    lr: 2.e-4
    momentum: 0.99
    weight_decay: 0.5
    nesterov: True

# Configuration for inference with a finetuned model
inference: !inference
  hd5_dataset: !hd5_dataset
    # Path to the h5 vocabulary to use
    vocab_fn: temp/datasets/bcu_enwiki_30000_bpe_vocab.h5
    # Path to the h5 dataset
    h5_fn: temp/datasets/gec_combined.bpe.h5
    # Which train/valid split to use from the h5 dataset
    valid_split_id: 0
  # Path to the finetuned model (checkpoint generated during finetune)
  pretrained_model: temp/Notebook/best_gec_combined_real_gen_mp_mn_gbl_ft_femb_gen800k_homo_1e4_300k.pkl
  # Force the computation to be on the cpu
  force_cpu: False
  # Beam width to use while decoding
  beam_width: 8
  # Path to the text file containing the sentence to correct
  source_fn: temp/jfleg/dev.src
  # Path for the output corrections
  hyp_fn: temp/jfleg/dev.spacy.lower.hyp
  # Max length of source sentence to consider (used to solve OOM errors)
  max_len: 125
  # Decoding maximum length based on the source sentence length
  max_len_scale: 1.5
  # From which line of source_fn to start the inference
  line_offset: 0
  # Beta used for random noising (https://www.aclweb.org/anthology/N18-1057.pdf)
  noising_beta: 0.
  # Decoding score temperature. Helps with noising by closing major gaps between candidat scores.
  temperature: 1.
  # Remove the '@@ ' in the output file
  remove_bpe_placeholder: True
  # Should output as 'source <split> decoded'
  output_parallel: False
  # Whether python prints should be buffered (usefull in cluster environments)
  output_buffering: False
  # Preprocess the sentences from source_fn with SpaCy tokenizer, BPE and to lowercase.
  preprocess: True


# Configuration for manual evaluation of a finetuned model with a list of sentences
manual_eval: !manual_eval
  hd5_dataset: !hd5_dataset
    # Path to the h5 vocabulary to use
    vocab_fn: temp/models/bcu_enwiki_50k_mf2_s0_vocab.h5
    # Path to the h5 dataset
    h5_fn: temp/datasets/obw.h5
    # Which train/valid split to use from the h5 dataset
    valid_split_id: 0
  # Path to the finetuned model (checkpoint generated during finetune)
  pretrained_model: temp/models/current_bcuenwiki_stream_pretrained.pkl
  # Force the computation to be on the cpu
  force_cpu: True
  # Beam width to use while decoding
  beam_width: 8
  # Decoding maximum length based on the source sentence length
  max_len_scale: 1.5
  # Sentences to correct. Number of dirty and clean should match.
  # 'dirty' should contain the sentence to correct and the associated 'clean' should be the expected output
  sample_corrections: !sample_corrections
    dirty:
      - "i am writing to reply your letter you wrote me on 10 June ."
      - "i hope quickly hearing from you ."
      - "the weather will be no cold doesn 't it ?"
      - "normally i enjoy really much shopping ."
      - "i 'm no more a young girl ."
      - "how much do i have to bring the money ?"
      - "i done it since i remember me ."
      - "don 't forget to wear the socks to get the straight suck"
      - 'Kaz is a man of culture i see . ?'
      - 'on the other hand , the theatre restaurant was closed because unknown reasons .'
      - 'i would like some kind of explanation and receive my money back .'
      - 'some countries are having difficulties in managing a place to live for their citizen as they tend to get overpopulated .'
      - 'on one hand more and more virus and hack can access personal computers , so the secret data and documents may be stolen .'
      - "i 'm keen on mountain biking since three years ago ."
      - "nobody won 't regret it ."
      - "i don 't recommend it to children lower than thirteen years old ,"
    clean:
      - "no matter what kind of policy the japanese government formulates , ultimately it would end up being criticized ."
      - "i hope to hear from you soon"
      - "the weather won 't be cold , will it ?"
      - "normally i really enjoy shopping ."
      - "i 'm not a young girl anymore ."
      - "how much money do i have to bring ?"
      - "i have been doing it for as long as i can remember ."
      - 'idk haha'
      - 'anyone got some stuff to show me ?'
      - 'in addition , the theatre restaurant was closed for unknown reasons .'
      - 'i would like some kind of explanation and to get my money back .'
      - 'some countries are having difficulties in managing to get a place to live for their citizens as they tend to get overpopulated .'
      - 'on one hand more and more viruses and hackers can access personal computers , so secret data and documents may be stolen .'
      - 'i have been keen on mountain biking since three years ago .'
      - "nobody will regret it ."
      - "i don 't recommend it to children under thirteen years old ,"